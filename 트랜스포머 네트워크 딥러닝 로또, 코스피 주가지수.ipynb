{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN4VFjn6OQP9uMzEZ/WQgZ8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#로또번호 예측 트랜스포머\n","\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import DataLoader\n","import numpy as np\n","\n","# CSV 파일 불러오기\n","data = pd.read_csv('lotto.csv')\n","\n","# 데이터 전처리\n","scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(data.iloc[:, 1:].values)  # 날짜 열 제외, 수치 데이터 스케일링\n","\n","# 훈련 데이터 준비\n","train_data = scaled_data[:-6]  # 마지막 6개 행 제외 (마지막 5개 데이터와 상금 데이터)\n","train_inputs = torch.tensor(train_data[:-1]).unsqueeze(0).float()  # 입력 데이터\n","train_labels = torch.tensor(train_data[1:]).unsqueeze(0).float()  # 출력 데이터\n","\n","\n","# 데이터셋 클래스 정의\n","class LottoDataset(torch.utils.data.Dataset):\n","    def __init__(self, inputs, labels):\n","        self.inputs = inputs\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, index):\n","        return self.inputs[index], self.labels[index]\n","\n","# 트랜스포머 모델 정의\n","class LottoTransformer(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers, num_heads):\n","        super(LottoTransformer, self).__init__()\n","        self.encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(input_size, num_heads, hidden_size),\n","            num_layers\n","        )\n","        self.decoder = nn.Linear(input_size, output_size)\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","# 하이퍼파라미터 설정\n","input_size = train_inputs.shape[-1]\n","hidden_size = 64\n","output_size = train_labels.shape[-1]\n","num_layers = 4\n","num_heads = 4\n","num_epochs = 100\n","learning_rate = 0.001\n","batch_size = 32\n","\n","# 데이터 로더 초기화\n","dataset = LottoDataset(train_inputs, train_labels)\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# 모델 초기화\n","model = LottoTransformer(input_size, hidden_size, output_size, num_layers, num_heads)\n","\n","# 손실 함수와 옵티마이저 정의\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# 모델 훈련\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch_inputs, batch_labels in data_loader:\n","        optimizer.zero_grad()\n","        outputs = model(batch_inputs)\n","        loss = criterion(outputs, batch_labels)\n","        loss.backward()\n","        optimizer.step()\n","    if (epoch+1) % 10 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","# 테스트 데이터 준비\n","test_inputs = torch.tensor(scaled_data[-3:-1]).unsqueeze(0).float()  # 2022년 12월 데이터 입력\n","\n","# 모델 예측\n","model.eval()\n","with torch.no_grad():\n","    predicted = model(test_inputs)\n","\n","# 스케일링 역변환\n","predicted = scaler.inverse_transform(predicted.squeeze(0).numpy())\n","\n","# 예측 결과 출력\n","lotto_numbers_prediction = predicted[-1].astype(int)[:-5]  # 마지막 5개 원소 제외\n","print(f'2023년 6월 로또 번호 예측: {lotto_numbers_prediction}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ftAIsmkmd2e8","executionInfo":{"status":"ok","timestamp":1686677558213,"user_tz":-540,"elapsed":1621,"user":{"displayName":"김민우","userId":"15651798176472178488"}},"outputId":"0e238731-60db-4faa-a1b0-3918187cbcc1"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [10/100], Loss: 0.1997\n","Epoch [20/100], Loss: 0.1042\n","Epoch [30/100], Loss: 0.0634\n","Epoch [40/100], Loss: 0.0479\n","Epoch [50/100], Loss: 0.0443\n","Epoch [60/100], Loss: 0.0427\n","Epoch [70/100], Loss: 0.0418\n","Epoch [80/100], Loss: 0.0408\n","Epoch [90/100], Loss: 0.0396\n","Epoch [100/100], Loss: 0.0379\n","2023년 6월 로또 번호 예측: [ 7 12 20 26 32 39 24]\n"]}]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Gq1eDSmJ4wv","executionInfo":{"status":"ok","timestamp":1686677575158,"user_tz":-540,"elapsed":16949,"user":{"displayName":"김민우","userId":"15651798176472178488"}},"outputId":"19e1d6d5-7bcd-4429-d161-743f73d3671d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0222\n","Epoch [200/1000], Loss: 0.0098\n","Epoch [300/1000], Loss: 0.0069\n","Epoch [400/1000], Loss: 0.0054\n","Epoch [500/1000], Loss: 0.0046\n","Epoch [600/1000], Loss: 0.0039\n","Epoch [700/1000], Loss: 0.0034\n","Epoch [800/1000], Loss: 0.0031\n","Epoch [900/1000], Loss: 0.0027\n","Epoch [1000/1000], Loss: 0.0025\n","2023년 4월의 코스피지수 예측: 2218.59\n"]}],"source":["#코스피 주가지수 예측 트랜스포머 (기본)\n","\n","# CSV 파일 불러오기\n","data = pd.read_csv('data100.csv')\n","\n","# 데이터 전처리\n","scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(data.iloc[:, 1:].values)  # 날짜 열 제외, 수치 데이터 스케일링\n","\n","# 훈련 데이터 준비\n","train_data = scaled_data[:-1]  # 마지막 행 제외\n","train_inputs = torch.tensor(train_data[:-1]).unsqueeze(0).float()  # 입력 데이터\n","train_labels = torch.tensor(train_data[1:]).unsqueeze(0).float()  # 출력 데이터\n","\n","# 트랜스포머 모델 정의\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers, num_heads):\n","        super(TransformerModel, self).__init__()\n","        self.encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(input_size, num_heads, hidden_size),\n","            num_layers\n","        )\n","        self.decoder = nn.Linear(input_size, output_size)\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","# 하이퍼파라미터 설정\n","input_size = train_inputs.shape[-1]\n","hidden_size = 64\n","output_size = train_labels.shape[-1]\n","num_layers = 6\n","num_heads = 4\n","num_epochs = 1000\n","learning_rate = 0.001\n","\n","# 모델 초기화\n","model = TransformerModel(input_size, hidden_size, output_size, num_layers, num_heads)\n","\n","# 손실 함수와 옵티마이저 정의\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# 모델 훈련\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(train_inputs)\n","    loss = criterion(outputs, train_labels)\n","    loss.backward()\n","    optimizer.step()\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","# 테스트 데이터 준비\n","test_inputs = torch.tensor(scaled_data[-2]).unsqueeze(0).float()  # 2022년 12월 데이터 입력\n","\n","# 모델 예측\n","model.eval()\n","with torch.no_grad():\n","    predicted = model(test_inputs)\n","\n","# 스케일링 역변환\n","predicted = scaler.inverse_transform(predicted.squeeze().unsqueeze(0).numpy())\n","\n","# 예측 결과 출력\n","kospis_index_prediction = predicted[0][-1]  # 2023년 1월의 코스피지수\n","\n","print(f'2023년 4월의 코스피지수 예측: {kospis_index_prediction:.2f}')\n"]},{"cell_type":"code","source":["#코스피 주가지수 예측 트랜스포머 (10회 실행 후 평균 및 표준편차)\n","\n","# CSV 파일 불러오기\n","data = pd.read_csv('data100.csv')\n","\n","# 데이터 전처리\n","scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(data.iloc[:, 1:].values)  # 날짜 열 제외, 수치 데이터 스케일링\n","\n","# 훈련 데이터 준비\n","train_data = scaled_data[:-1]  # 마지막 행 제외\n","train_inputs = torch.tensor(train_data[:-1]).unsqueeze(0).float()  # 입력 데이터\n","train_labels = torch.tensor(train_data[1:]).unsqueeze(0).float()  # 출력 데이터\n","\n","# 트랜스포머 모델 정의\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers, num_heads):\n","        super(TransformerModel, self).__init__()\n","        self.encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(input_size, num_heads, hidden_size),\n","            num_layers\n","        )\n","        self.decoder = nn.Linear(input_size, output_size)\n","\n","    def forward(self, x):\n","        encoded = self.encoder(x)\n","        decoded = self.decoder(encoded)\n","        return decoded\n","\n","# 하이퍼파라미터 설정\n","input_size = train_inputs.shape[-1]\n","hidden_size = 64\n","output_size = train_labels.shape[-1]\n","num_layers = 6\n","num_heads = 4\n","num_runs = 10  # 실행 횟수\n","num_epochs = 1000 #찾는 횟수\n","learning_rate = 0.001\n","\n","predictions = []  # 예측 결과를 저장할 리스트\n","\n","for _ in range(num_runs):\n","    # 모델 초기화\n","    model = TransformerModel(input_size, hidden_size, output_size, num_layers, num_heads)\n","\n","    # 손실 함수와 옵티마이저 정의\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    # 모델 훈련\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(train_inputs)\n","        loss = criterion(outputs, train_labels)\n","        loss.backward()\n","        optimizer.step()\n","        if (epoch+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","    # 테스트 데이터 준비\n","    test_inputs = torch.tensor(scaled_data[-2]).unsqueeze(0).float()  # 2022년 12월 데이터 입력\n","\n","    # 모델 예측\n","    model.eval()\n","    with torch.no_grad():\n","        predicted = model(test_inputs)\n","\n","    # 스케일링 역변환\n","    predicted = scaler.inverse_transform(predicted.squeeze().unsqueeze(0).numpy())\n","\n","    # 예측 결과 저장\n","    kospis_index_prediction = predicted[0][-1]  # 2023년 1월의 코스피지수\n","    predictions.append(kospis_index_prediction)\n","\n","# 예측 결과 평균 및 표준 편차 계산\n","predictions = np.array(predictions)\n","mean_prediction = np.mean(predictions)\n","std_prediction = np.std(predictions)\n","\n","print(f'평균 예측값: {mean_prediction:.2f}')\n","print(f'표준 편차: {std_prediction:.2f}')\n"],"metadata":{"id":"uNwVx2M4Mc0z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686677737631,"user_tz":-540,"elapsed":162500,"user":{"displayName":"김민우","userId":"15651798176472178488"}},"outputId":"37d1f0eb-7702-4eed-8b81-afdb622dc464"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0170\n","Epoch [200/1000], Loss: 0.0099\n","Epoch [300/1000], Loss: 0.0069\n","Epoch [400/1000], Loss: 0.0055\n","Epoch [500/1000], Loss: 0.0045\n","Epoch [600/1000], Loss: 0.0040\n","Epoch [700/1000], Loss: 0.0034\n","Epoch [800/1000], Loss: 0.0031\n","Epoch [900/1000], Loss: 0.0027\n","Epoch [1000/1000], Loss: 0.0026\n","Epoch [100/1000], Loss: 0.0228\n","Epoch [200/1000], Loss: 0.0113\n","Epoch [300/1000], Loss: 0.0077\n","Epoch [400/1000], Loss: 0.0059\n","Epoch [500/1000], Loss: 0.0047\n","Epoch [600/1000], Loss: 0.0043\n","Epoch [700/1000], Loss: 0.0036\n","Epoch [800/1000], Loss: 0.0033\n","Epoch [900/1000], Loss: 0.0030\n","Epoch [1000/1000], Loss: 0.0027\n","Epoch [100/1000], Loss: 0.0190\n","Epoch [200/1000], Loss: 0.0093\n","Epoch [300/1000], Loss: 0.0067\n","Epoch [400/1000], Loss: 0.0051\n","Epoch [500/1000], Loss: 0.0044\n","Epoch [600/1000], Loss: 0.0038\n","Epoch [700/1000], Loss: 0.0034\n","Epoch [800/1000], Loss: 0.0031\n","Epoch [900/1000], Loss: 0.0028\n","Epoch [1000/1000], Loss: 0.0026\n","Epoch [100/1000], Loss: 0.0192\n","Epoch [200/1000], Loss: 0.0092\n","Epoch [300/1000], Loss: 0.0066\n","Epoch [400/1000], Loss: 0.0053\n","Epoch [500/1000], Loss: 0.0044\n","Epoch [600/1000], Loss: 0.0038\n","Epoch [700/1000], Loss: 0.0033\n","Epoch [800/1000], Loss: 0.0030\n","Epoch [900/1000], Loss: 0.0027\n","Epoch [1000/1000], Loss: 0.0025\n","Epoch [100/1000], Loss: 0.0220\n","Epoch [200/1000], Loss: 0.0110\n","Epoch [300/1000], Loss: 0.0074\n","Epoch [400/1000], Loss: 0.0058\n","Epoch [500/1000], Loss: 0.0047\n","Epoch [600/1000], Loss: 0.0041\n","Epoch [700/1000], Loss: 0.0035\n","Epoch [800/1000], Loss: 0.0031\n","Epoch [900/1000], Loss: 0.0028\n","Epoch [1000/1000], Loss: 0.0025\n","Epoch [100/1000], Loss: 0.0240\n","Epoch [200/1000], Loss: 0.0112\n","Epoch [300/1000], Loss: 0.0076\n","Epoch [400/1000], Loss: 0.0060\n","Epoch [500/1000], Loss: 0.0050\n","Epoch [600/1000], Loss: 0.0043\n","Epoch [700/1000], Loss: 0.0037\n","Epoch [800/1000], Loss: 0.0034\n","Epoch [900/1000], Loss: 0.0030\n","Epoch [1000/1000], Loss: 0.0028\n","Epoch [100/1000], Loss: 0.0203\n","Epoch [200/1000], Loss: 0.0112\n","Epoch [300/1000], Loss: 0.0075\n","Epoch [400/1000], Loss: 0.0058\n","Epoch [500/1000], Loss: 0.0046\n","Epoch [600/1000], Loss: 0.0041\n","Epoch [700/1000], Loss: 0.0035\n","Epoch [800/1000], Loss: 0.0032\n","Epoch [900/1000], Loss: 0.0030\n","Epoch [1000/1000], Loss: 0.0027\n","Epoch [100/1000], Loss: 0.0209\n","Epoch [200/1000], Loss: 0.0102\n","Epoch [300/1000], Loss: 0.0068\n","Epoch [400/1000], Loss: 0.0053\n","Epoch [500/1000], Loss: 0.0044\n","Epoch [600/1000], Loss: 0.0037\n","Epoch [700/1000], Loss: 0.0033\n","Epoch [800/1000], Loss: 0.0031\n","Epoch [900/1000], Loss: 0.0027\n","Epoch [1000/1000], Loss: 0.0025\n","Epoch [100/1000], Loss: 0.0200\n","Epoch [200/1000], Loss: 0.0103\n","Epoch [300/1000], Loss: 0.0074\n","Epoch [400/1000], Loss: 0.0058\n","Epoch [500/1000], Loss: 0.0047\n","Epoch [600/1000], Loss: 0.0040\n","Epoch [700/1000], Loss: 0.0035\n","Epoch [800/1000], Loss: 0.0032\n","Epoch [900/1000], Loss: 0.0030\n","Epoch [1000/1000], Loss: 0.0027\n","Epoch [100/1000], Loss: 0.0226\n","Epoch [200/1000], Loss: 0.0103\n","Epoch [300/1000], Loss: 0.0074\n","Epoch [400/1000], Loss: 0.0059\n","Epoch [500/1000], Loss: 0.0048\n","Epoch [600/1000], Loss: 0.0041\n","Epoch [700/1000], Loss: 0.0035\n","Epoch [800/1000], Loss: 0.0032\n","Epoch [900/1000], Loss: 0.0028\n","Epoch [1000/1000], Loss: 0.0027\n","평균 예측값: 2340.92\n","표준 편차: 75.23\n"]}]},{"cell_type":"code","source":["#가중치가 반영된 코스피 주가지수 예측 트랜스포머\n","\n","# CSV 파일 불러오기\n","data = pd.read_csv('data100.csv')\n","\n","# 데이터 전처리\n","scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(data.iloc[:, 1:].values)  # 날짜 열 제외, 수치 데이터 스케일링\n","\n","# 훈련 데이터 준비\n","train_data = scaled_data[:-1]  # 마지막 행 제외\n","train_inputs = torch.tensor(train_data[:-1]).unsqueeze(0).float()  # 입력 데이터\n","train_labels = torch.tensor(train_data[1:]).unsqueeze(0).float()  # 출력 데이터\n","\n","# 트랜스포머 모델 정의\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers, num_heads):\n","        super(TransformerModel, self).__init__()\n","        self.encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(hidden_size, num_heads, hidden_size),\n","            num_layers\n","        )\n","        self.decoder = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, weights):\n","        encoded = self.encoder(x)\n","        weighted_encoded = encoded * weights.unsqueeze(0).unsqueeze(1).unsqueeze(1).expand(-1, encoded.size(1), -1, -1).transpose(0, 1)\n","        decoded = self.decoder(weighted_encoded)\n","        return decoded\n","\n","# 하이퍼파라미터 설정\n","input_size = train_inputs.shape[-1]\n","hidden_size = 64\n","output_size = train_labels.shape[-1]\n","num_layers = 6\n","num_heads = 4\n","num_epochs = 1000\n","learning_rate = 0.001\n","\n","# 모델 초기화\n","model = TransformerModel(input_size, hidden_size, output_size, num_layers, num_heads)\n","\n","# 손실 함수와 옵티마이저 정의\n","criterion = nn.MSELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# 가중치 설정\n","weights = torch.randn(hidden_size)\n","\n","# 모델 훈련\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    outputs = model(train_inputs, weights)  # 가중치 전달\n","    loss = criterion(outputs, train_labels)\n","    loss.backward()\n","    optimizer.step()\n","    if (epoch+1) % 100 == 0:\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","# 테스트 데이터 준비\n","test_inputs = torch.tensor(scaled_data[-2]).unsqueeze(0).float()  # 2022년 12월 데이터 입력\n","\n","# 모델 예측\n","model.eval()\n","with torch.no_grad():\n","    predicted = model(test_inputs, weights)  # 가중치 전달\n","\n","# 스케일링 역변환\n","predicted = scaler.inverse_transform(predicted.squeeze(0).transpose(1, 3).reshape(-1, predicted.shape[-1])).reshape(-1, predicted.shape[-1])\n","\n","# 예측 결과 출력\n","kospis_index_prediction = predicted[-1, -1]\n","print(f'Predicted KOSPI Index: {kospis_index_prediction:.2f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lK7fRjQehQLO","executionInfo":{"status":"ok","timestamp":1686677786730,"user_tz":-540,"elapsed":49135,"user":{"displayName":"김민우","userId":"15651798176472178488"}},"outputId":"b1951d78-c36a-473c-f072-df1e22e42dce"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0268\n","Epoch [200/1000], Loss: 0.0136\n","Epoch [300/1000], Loss: 0.0093\n","Epoch [400/1000], Loss: 0.0071\n","Epoch [500/1000], Loss: 0.0057\n","Epoch [600/1000], Loss: 0.0051\n","Epoch [700/1000], Loss: 0.0043\n","Epoch [800/1000], Loss: 0.0040\n","Epoch [900/1000], Loss: 0.0035\n","Epoch [1000/1000], Loss: 0.0031\n","Predicted KOSPI Index: 2266.62\n"]}]},{"cell_type":"code","source":["#코스피 주가지수의 증감 퍼센트를 확인하는 트랜스포머\n","\n","# CSV 파일 불러오기\n","data = pd.read_csv('percent.csv')\n","\n","# 데이터 전처리\n","scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(data.iloc[:, 1:].values)  # 날짜 열 제외, 수치 데이터 스케일링\n","\n","# 훈련 데이터 준비\n","train_data = scaled_data[:-1]  # 마지막 행 제외\n","train_inputs = torch.tensor(train_data[:-1]).unsqueeze(0).float()  # 입력 데이터\n","train_labels = torch.tensor(train_data[1:]).unsqueeze(0).float()  # 출력 데이터\n","\n","# 트랜스포머 모델 정의\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers, num_heads):\n","        super(TransformerModel, self).__init__()\n","        self.encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(hidden_size, num_heads, hidden_size),\n","            num_layers\n","        )\n","        self.decoder = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, weights):\n","        encoded = self.encoder(x)\n","        weighted_encoded = encoded * weights.unsqueeze(0).unsqueeze(1).unsqueeze(1).expand(-1, encoded.size(1), -1, -1).transpose(0, 1)\n","        decoded = self.decoder(weighted_encoded)\n","        return decoded\n","\n","input_size = train_inputs.shape[-1]\n","hidden_size = 64\n","output_size = train_labels.shape[-1]\n","num_layers = 6\n","num_heads = 4\n","num_epochs = 1000\n","learning_rate = 0.001\n","num_runs = 10\n","\n","predictions = []\n","\n","for _ in range(num_runs):\n","\n","    model = TransformerModel(input_size, hidden_size, output_size, num_layers, num_heads)\n","\n","\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","    weights = torch.randn(hidden_size)\n","\n","    # 모델 훈련\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(train_inputs, weights)  # 가중치 전달\n","        loss = criterion(outputs, train_labels)\n","        loss.backward()\n","        optimizer.step()\n","        if (epoch+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","    # 테스트 데이터 준비\n","    test_inputs = torch.tensor(scaled_data[-2]).unsqueeze(0).float()  # 2022년 12월 데이터 입력\n","\n","    # 모델 예측\n","    model.eval()\n","    with torch.no_grad():\n","        predicted = model(test_inputs, weights)  # 가중치 전달\n","\n","    # 스케일링 역변환\n","    predicted = scaler.inverse_transform(predicted.squeeze(0).transpose(1, 3).reshape(-1, predicted.shape[-1])).reshape(-1, predicted.shape[-1])\n","\n","    # 예측 결과 저장\n","    predictions.append(predicted[-1, -1])\n","\n","# 예측 결과의 평균과 표준 편차 계산\n","predictions = np.array(predictions)\n","mean_prediction = predictions.mean()\n","std_prediction = predictions.std()\n","\n","print(f'Mean of Predicted KOSPI Index: {mean_prediction:.2f}')\n","print(f'Standard Deviation of Predicted KOSPI Index: {std_prediction:.2f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2Y0H5FuuTjT","executionInfo":{"status":"ok","timestamp":1686678275129,"user_tz":-540,"elapsed":488436,"user":{"displayName":"김민우","userId":"15651798176472178488"}},"outputId":"cd56a8c6-baf1-4990-9478-15626f5d81e6"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0211\n","Epoch [200/1000], Loss: 0.0191\n","Epoch [300/1000], Loss: 0.0181\n","Epoch [400/1000], Loss: 0.0167\n","Epoch [500/1000], Loss: 0.0150\n","Epoch [600/1000], Loss: 0.0135\n","Epoch [700/1000], Loss: 0.0118\n","Epoch [800/1000], Loss: 0.0107\n","Epoch [900/1000], Loss: 0.0096\n","Epoch [1000/1000], Loss: 0.0086\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0220\n","Epoch [200/1000], Loss: 0.0201\n","Epoch [300/1000], Loss: 0.0190\n","Epoch [400/1000], Loss: 0.0180\n","Epoch [500/1000], Loss: 0.0164\n","Epoch [600/1000], Loss: 0.0148\n","Epoch [700/1000], Loss: 0.0131\n","Epoch [800/1000], Loss: 0.0117\n","Epoch [900/1000], Loss: 0.0101\n","Epoch [1000/1000], Loss: 0.0092\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0227\n","Epoch [200/1000], Loss: 0.0204\n","Epoch [300/1000], Loss: 0.0195\n","Epoch [400/1000], Loss: 0.0188\n","Epoch [500/1000], Loss: 0.0180\n","Epoch [600/1000], Loss: 0.0169\n","Epoch [700/1000], Loss: 0.0157\n","Epoch [800/1000], Loss: 0.0145\n","Epoch [900/1000], Loss: 0.0128\n","Epoch [1000/1000], Loss: 0.0118\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0212\n","Epoch [200/1000], Loss: 0.0198\n","Epoch [300/1000], Loss: 0.0191\n","Epoch [400/1000], Loss: 0.0181\n","Epoch [500/1000], Loss: 0.0168\n","Epoch [600/1000], Loss: 0.0147\n","Epoch [700/1000], Loss: 0.0128\n","Epoch [800/1000], Loss: 0.0111\n","Epoch [900/1000], Loss: 0.0098\n","Epoch [1000/1000], Loss: 0.0087\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0205\n","Epoch [200/1000], Loss: 0.0193\n","Epoch [300/1000], Loss: 0.0184\n","Epoch [400/1000], Loss: 0.0171\n","Epoch [500/1000], Loss: 0.0153\n","Epoch [600/1000], Loss: 0.0132\n","Epoch [700/1000], Loss: 0.0115\n","Epoch [800/1000], Loss: 0.0098\n","Epoch [900/1000], Loss: 0.0086\n","Epoch [1000/1000], Loss: 0.0077\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0227\n","Epoch [200/1000], Loss: 0.0205\n","Epoch [300/1000], Loss: 0.0193\n","Epoch [400/1000], Loss: 0.0181\n","Epoch [500/1000], Loss: 0.0170\n","Epoch [600/1000], Loss: 0.0160\n","Epoch [700/1000], Loss: 0.0148\n","Epoch [800/1000], Loss: 0.0134\n","Epoch [900/1000], Loss: 0.0125\n","Epoch [1000/1000], Loss: 0.0116\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0220\n","Epoch [200/1000], Loss: 0.0204\n","Epoch [300/1000], Loss: 0.0191\n","Epoch [400/1000], Loss: 0.0180\n","Epoch [500/1000], Loss: 0.0168\n","Epoch [600/1000], Loss: 0.0153\n","Epoch [700/1000], Loss: 0.0141\n","Epoch [800/1000], Loss: 0.0128\n","Epoch [900/1000], Loss: 0.0114\n","Epoch [1000/1000], Loss: 0.0104\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0207\n","Epoch [200/1000], Loss: 0.0194\n","Epoch [300/1000], Loss: 0.0182\n","Epoch [400/1000], Loss: 0.0172\n","Epoch [500/1000], Loss: 0.0155\n","Epoch [600/1000], Loss: 0.0140\n","Epoch [700/1000], Loss: 0.0124\n","Epoch [800/1000], Loss: 0.0111\n","Epoch [900/1000], Loss: 0.0096\n","Epoch [1000/1000], Loss: 0.0087\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0216\n","Epoch [200/1000], Loss: 0.0199\n","Epoch [300/1000], Loss: 0.0188\n","Epoch [400/1000], Loss: 0.0177\n","Epoch [500/1000], Loss: 0.0170\n","Epoch [600/1000], Loss: 0.0152\n","Epoch [700/1000], Loss: 0.0139\n","Epoch [800/1000], Loss: 0.0123\n","Epoch [900/1000], Loss: 0.0109\n","Epoch [1000/1000], Loss: 0.0098\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0214\n","Epoch [200/1000], Loss: 0.0198\n","Epoch [300/1000], Loss: 0.0188\n","Epoch [400/1000], Loss: 0.0177\n","Epoch [500/1000], Loss: 0.0170\n","Epoch [600/1000], Loss: 0.0156\n","Epoch [700/1000], Loss: 0.0145\n","Epoch [800/1000], Loss: 0.0130\n","Epoch [900/1000], Loss: 0.0119\n","Epoch [1000/1000], Loss: 0.0106\n","Mean of Predicted KOSPI Index: -0.94\n","Standard Deviation of Predicted KOSPI Index: 4.87\n"]}]},{"cell_type":"code","source":["#코스피 주가지수의 증감 퍼센트 및 가중치와 10회 실행 후 평균과 표준편차를 구하는 트랜스포머\n","\n","# CSV 파일 불러오기\n","data = pd.read_csv('percent.csv')\n","\n","# 데이터 전처리\n","scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(data.iloc[:, 1:].values)  # 날짜 열 제외, 수치 데이터 스케일링\n","\n","# 훈련 데이터 준비\n","train_data = scaled_data[:-1]  # 마지막 행 제외\n","train_inputs = torch.tensor(train_data[:-1]).unsqueeze(0).float()  # 입력 데이터\n","train_labels = torch.tensor(train_data[1:]).unsqueeze(0).float()  # 출력 데이터\n","\n","# 트랜스포머 모델 정의\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers, num_heads):\n","        super(TransformerModel, self).__init__()\n","        self.encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(hidden_size, num_heads, hidden_size),\n","            num_layers\n","        )\n","        self.decoder = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, weights):\n","        encoded = self.encoder(x)\n","        weighted_encoded = encoded * weights.unsqueeze(0).unsqueeze(1).unsqueeze(1).expand(-1, encoded.size(1), -1, -1).transpose(0, 1)\n","        decoded = self.decoder(weighted_encoded)\n","        return decoded\n","\n","input_size = train_inputs.shape[-1]\n","hidden_size = 64\n","output_size = train_labels.shape[-1]\n","num_layers = 6\n","num_heads = 4\n","num_epochs = 1000\n","learning_rate = 0.001\n","num_runs = 10\n","\n","predictions = []\n","\n","for _ in range(num_runs):\n","\n","    model = TransformerModel(input_size, hidden_size, output_size, num_layers, num_heads)\n","\n","\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","\n","    weights = torch.randn(hidden_size)\n","\n","    # 모델 훈련\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(train_inputs, weights)  # 가중치 전달\n","        loss = criterion(outputs, train_labels)\n","        loss.backward()\n","        optimizer.step()\n","        if (epoch+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","    # 테스트 데이터 준비\n","    test_inputs = torch.tensor(scaled_data[-2]).unsqueeze(0).float()  # 2022년 12월 데이터 입력\n","\n","    # 모델 예측\n","    model.eval()\n","    with torch.no_grad():\n","        predicted = model(test_inputs, weights)  # 가중치 전달\n","\n","    # 스케일링 역변환\n","    predicted = scaler.inverse_transform(predicted.squeeze(0).transpose(1, 3).reshape(-1, predicted.shape[-1])).reshape(-1, predicted.shape[-1])\n","\n","    # 예측 결과 저장\n","    predictions.append(predicted[-1, -1])\n","\n","# 예측 결과의 평균과 표준 편차 계산\n","predictions = np.array(predictions)\n","mean_prediction = predictions.mean()\n","std_prediction = predictions.std()\n","\n","print(f'Mean of Predicted KOSPI Index: {mean_prediction:.2f}')\n","print(f'Standard Deviation of Predicted KOSPI Index: {std_prediction:.2f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvtnlcNUuT5L","executionInfo":{"status":"ok","timestamp":1686678754891,"user_tz":-540,"elapsed":479797,"user":{"displayName":"김민우","userId":"15651798176472178488"}},"outputId":"9c067ae7-270d-4888-bfb0-a5570a8ef758"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0213\n","Epoch [200/1000], Loss: 0.0198\n","Epoch [300/1000], Loss: 0.0185\n","Epoch [400/1000], Loss: 0.0172\n","Epoch [500/1000], Loss: 0.0160\n","Epoch [600/1000], Loss: 0.0147\n","Epoch [700/1000], Loss: 0.0133\n","Epoch [800/1000], Loss: 0.0122\n","Epoch [900/1000], Loss: 0.0110\n","Epoch [1000/1000], Loss: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0203\n","Epoch [200/1000], Loss: 0.0192\n","Epoch [300/1000], Loss: 0.0185\n","Epoch [400/1000], Loss: 0.0170\n","Epoch [500/1000], Loss: 0.0152\n","Epoch [600/1000], Loss: 0.0137\n","Epoch [700/1000], Loss: 0.0124\n","Epoch [800/1000], Loss: 0.0110\n","Epoch [900/1000], Loss: 0.0099\n","Epoch [1000/1000], Loss: 0.0090\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0222\n","Epoch [200/1000], Loss: 0.0200\n","Epoch [300/1000], Loss: 0.0187\n","Epoch [400/1000], Loss: 0.0175\n","Epoch [500/1000], Loss: 0.0163\n","Epoch [600/1000], Loss: 0.0151\n","Epoch [700/1000], Loss: 0.0136\n","Epoch [800/1000], Loss: 0.0126\n","Epoch [900/1000], Loss: 0.0112\n","Epoch [1000/1000], Loss: 0.0099\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0223\n","Epoch [200/1000], Loss: 0.0200\n","Epoch [300/1000], Loss: 0.0191\n","Epoch [400/1000], Loss: 0.0188\n","Epoch [500/1000], Loss: 0.0183\n","Epoch [600/1000], Loss: 0.0176\n","Epoch [700/1000], Loss: 0.0168\n","Epoch [800/1000], Loss: 0.0159\n","Epoch [900/1000], Loss: 0.0148\n","Epoch [1000/1000], Loss: 0.0139\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0205\n","Epoch [200/1000], Loss: 0.0192\n","Epoch [300/1000], Loss: 0.0182\n","Epoch [400/1000], Loss: 0.0166\n","Epoch [500/1000], Loss: 0.0153\n","Epoch [600/1000], Loss: 0.0136\n","Epoch [700/1000], Loss: 0.0121\n","Epoch [800/1000], Loss: 0.0107\n","Epoch [900/1000], Loss: 0.0095\n","Epoch [1000/1000], Loss: 0.0086\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0210\n","Epoch [200/1000], Loss: 0.0197\n","Epoch [300/1000], Loss: 0.0183\n","Epoch [400/1000], Loss: 0.0169\n","Epoch [500/1000], Loss: 0.0157\n","Epoch [600/1000], Loss: 0.0145\n","Epoch [700/1000], Loss: 0.0132\n","Epoch [800/1000], Loss: 0.0117\n","Epoch [900/1000], Loss: 0.0105\n","Epoch [1000/1000], Loss: 0.0092\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0219\n","Epoch [200/1000], Loss: 0.0197\n","Epoch [300/1000], Loss: 0.0189\n","Epoch [400/1000], Loss: 0.0175\n","Epoch [500/1000], Loss: 0.0165\n","Epoch [600/1000], Loss: 0.0152\n","Epoch [700/1000], Loss: 0.0139\n","Epoch [800/1000], Loss: 0.0126\n","Epoch [900/1000], Loss: 0.0113\n","Epoch [1000/1000], Loss: 0.0101\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0201\n","Epoch [200/1000], Loss: 0.0191\n","Epoch [300/1000], Loss: 0.0179\n","Epoch [400/1000], Loss: 0.0162\n","Epoch [500/1000], Loss: 0.0143\n","Epoch [600/1000], Loss: 0.0121\n","Epoch [700/1000], Loss: 0.0105\n","Epoch [800/1000], Loss: 0.0093\n","Epoch [900/1000], Loss: 0.0082\n","Epoch [1000/1000], Loss: 0.0074\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0219\n","Epoch [200/1000], Loss: 0.0201\n","Epoch [300/1000], Loss: 0.0186\n","Epoch [400/1000], Loss: 0.0182\n","Epoch [500/1000], Loss: 0.0171\n","Epoch [600/1000], Loss: 0.0158\n","Epoch [700/1000], Loss: 0.0145\n","Epoch [800/1000], Loss: 0.0132\n","Epoch [900/1000], Loss: 0.0118\n","Epoch [1000/1000], Loss: 0.0108\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0219\n","Epoch [200/1000], Loss: 0.0198\n","Epoch [300/1000], Loss: 0.0187\n","Epoch [400/1000], Loss: 0.0172\n","Epoch [500/1000], Loss: 0.0158\n","Epoch [600/1000], Loss: 0.0149\n","Epoch [700/1000], Loss: 0.0133\n","Epoch [800/1000], Loss: 0.0121\n","Epoch [900/1000], Loss: 0.0109\n","Epoch [1000/1000], Loss: 0.0100\n","Mean of Predicted KOSPI Index: -2.42\n","Standard Deviation of Predicted KOSPI Index: 3.32\n"]}]},{"cell_type":"code","source":["#위 코드에서 loss함수를 Huber Loss로 대체한 코드\n","\n","# CSV 파일 불러오기\n","data = pd.read_csv('percent.csv')\n","\n","# 데이터 전처리\n","scaler = MinMaxScaler()\n","scaled_data = scaler.fit_transform(data.iloc[:, 1:].values)  # 날짜 열 제외, 수치 데이터 스케일링\n","\n","# 훈련 데이터 준비\n","train_data = scaled_data[:-1]  # 마지막 행 제외\n","train_inputs = torch.tensor(train_data[:-1]).unsqueeze(0).float()  # 입력 데이터\n","train_labels = torch.tensor(train_data[1:]).unsqueeze(0).float()  # 출력 데이터\n","\n","# 트랜스포머 모델 정의\n","class TransformerModel(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers, num_heads):\n","        super(TransformerModel, self).__init__()\n","        self.encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(hidden_size, num_heads, hidden_size),\n","            num_layers\n","        )\n","        self.decoder = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, weights):\n","        encoded = self.encoder(x)\n","        weighted_encoded = encoded * weights.unsqueeze(0).unsqueeze(1).unsqueeze(1).expand(-1, encoded.size(1), -1, -1).transpose(0, 1)\n","        decoded = self.decoder(weighted_encoded)\n","        return decoded\n","\n","input_size = train_inputs.shape[-1]\n","hidden_size = 64\n","output_size = train_labels.shape[-1]\n","num_layers = 6\n","num_heads = 4\n","num_epochs = 1000\n","learning_rate = 0.001\n","num_runs = 10\n","\n","predictions = []\n","\n","for _ in range(num_runs):\n","\n","    model = TransformerModel(input_size, hidden_size, output_size, num_layers, num_heads)\n","\n","    criterion = nn.SmoothL1Loss()  # Huber Loss로 대체\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","    weights = torch.randn(hidden_size)\n","\n","    # 모델 훈련\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","        outputs = model(train_inputs, weights)  # 가중치 전달\n","        loss = criterion(outputs, train_labels)\n","        loss.backward()\n","        optimizer.step()\n","        if (epoch+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n","\n","    # 테스트 데이터 준비\n","    test_inputs = torch.tensor(scaled_data[-2]).unsqueeze(0).float()  # 2022년 12월 데이터 입력\n","\n","    # 모델 예측\n","    model.eval()\n","    with torch.no_grad():\n","        predicted = model(test_inputs, weights)  # 가중치 전달\n","\n","    # 스케일링 역변환\n","    predicted = scaler.inverse_transform(predicted.squeeze(0).transpose(1, 3).reshape(-1, predicted.shape[-1])).reshape(-1, predicted.shape[-1])\n","\n","    # 예측 결과 저장\n","    predictions.append(predicted[-1, -1])\n","\n","# 예측 결과의 평균과 표준 편차 계산\n","predictions = np.array(predictions)\n","mean_prediction = predictions.mean()\n","std_prediction = predictions.std()\n","\n","print(f'Mean of Predicted KOSPI Index: {mean_prediction:.2f}')\n","print(f'Standard Deviation of Predicted KOSPI Index: {std_prediction:.2f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_EoTNrXIfYP","executionInfo":{"status":"ok","timestamp":1686679261810,"user_tz":-540,"elapsed":506946,"user":{"displayName":"김민우","userId":"15651798176472178488"}},"outputId":"c2fc8a3b-082e-4a8c-ce5b-aa2e84f6252f"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0108\n","Epoch [200/1000], Loss: 0.0100\n","Epoch [300/1000], Loss: 0.0092\n","Epoch [400/1000], Loss: 0.0086\n","Epoch [500/1000], Loss: 0.0077\n","Epoch [600/1000], Loss: 0.0069\n","Epoch [700/1000], Loss: 0.0061\n","Epoch [800/1000], Loss: 0.0055\n","Epoch [900/1000], Loss: 0.0049\n","Epoch [1000/1000], Loss: 0.0044\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0106\n","Epoch [200/1000], Loss: 0.0098\n","Epoch [300/1000], Loss: 0.0093\n","Epoch [400/1000], Loss: 0.0089\n","Epoch [500/1000], Loss: 0.0085\n","Epoch [600/1000], Loss: 0.0078\n","Epoch [700/1000], Loss: 0.0068\n","Epoch [800/1000], Loss: 0.0060\n","Epoch [900/1000], Loss: 0.0052\n","Epoch [1000/1000], Loss: 0.0047\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0106\n","Epoch [200/1000], Loss: 0.0099\n","Epoch [300/1000], Loss: 0.0095\n","Epoch [400/1000], Loss: 0.0093\n","Epoch [500/1000], Loss: 0.0085\n","Epoch [600/1000], Loss: 0.0078\n","Epoch [700/1000], Loss: 0.0068\n","Epoch [800/1000], Loss: 0.0061\n","Epoch [900/1000], Loss: 0.0055\n","Epoch [1000/1000], Loss: 0.0050\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0109\n","Epoch [200/1000], Loss: 0.0099\n","Epoch [300/1000], Loss: 0.0092\n","Epoch [400/1000], Loss: 0.0084\n","Epoch [500/1000], Loss: 0.0076\n","Epoch [600/1000], Loss: 0.0068\n","Epoch [700/1000], Loss: 0.0063\n","Epoch [800/1000], Loss: 0.0055\n","Epoch [900/1000], Loss: 0.0049\n","Epoch [1000/1000], Loss: 0.0046\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0103\n","Epoch [200/1000], Loss: 0.0097\n","Epoch [300/1000], Loss: 0.0091\n","Epoch [400/1000], Loss: 0.0087\n","Epoch [500/1000], Loss: 0.0080\n","Epoch [600/1000], Loss: 0.0072\n","Epoch [700/1000], Loss: 0.0062\n","Epoch [800/1000], Loss: 0.0054\n","Epoch [900/1000], Loss: 0.0047\n","Epoch [1000/1000], Loss: 0.0042\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0111\n","Epoch [200/1000], Loss: 0.0101\n","Epoch [300/1000], Loss: 0.0095\n","Epoch [400/1000], Loss: 0.0090\n","Epoch [500/1000], Loss: 0.0082\n","Epoch [600/1000], Loss: 0.0074\n","Epoch [700/1000], Loss: 0.0065\n","Epoch [800/1000], Loss: 0.0058\n","Epoch [900/1000], Loss: 0.0051\n","Epoch [1000/1000], Loss: 0.0046\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0109\n","Epoch [200/1000], Loss: 0.0100\n","Epoch [300/1000], Loss: 0.0096\n","Epoch [400/1000], Loss: 0.0090\n","Epoch [500/1000], Loss: 0.0086\n","Epoch [600/1000], Loss: 0.0079\n","Epoch [700/1000], Loss: 0.0072\n","Epoch [800/1000], Loss: 0.0065\n","Epoch [900/1000], Loss: 0.0058\n","Epoch [1000/1000], Loss: 0.0053\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0109\n","Epoch [200/1000], Loss: 0.0099\n","Epoch [300/1000], Loss: 0.0093\n","Epoch [400/1000], Loss: 0.0086\n","Epoch [500/1000], Loss: 0.0079\n","Epoch [600/1000], Loss: 0.0070\n","Epoch [700/1000], Loss: 0.0063\n","Epoch [800/1000], Loss: 0.0056\n","Epoch [900/1000], Loss: 0.0050\n","Epoch [1000/1000], Loss: 0.0045\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0111\n","Epoch [200/1000], Loss: 0.0101\n","Epoch [300/1000], Loss: 0.0094\n","Epoch [400/1000], Loss: 0.0091\n","Epoch [500/1000], Loss: 0.0087\n","Epoch [600/1000], Loss: 0.0082\n","Epoch [700/1000], Loss: 0.0075\n","Epoch [800/1000], Loss: 0.0070\n","Epoch [900/1000], Loss: 0.0063\n","Epoch [1000/1000], Loss: 0.0058\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:928: UserWarning: Using a target size (torch.Size([1, 217, 64])) that is different to the input size (torch.Size([217, 1, 217, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.smooth_l1_loss(input, target, reduction=self.reduction, beta=self.beta)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [100/1000], Loss: 0.0109\n","Epoch [200/1000], Loss: 0.0101\n","Epoch [300/1000], Loss: 0.0094\n","Epoch [400/1000], Loss: 0.0090\n","Epoch [500/1000], Loss: 0.0086\n","Epoch [600/1000], Loss: 0.0079\n","Epoch [700/1000], Loss: 0.0071\n","Epoch [800/1000], Loss: 0.0064\n","Epoch [900/1000], Loss: 0.0057\n","Epoch [1000/1000], Loss: 0.0051\n","Mean of Predicted KOSPI Index: -2.96\n","Standard Deviation of Predicted KOSPI Index: 2.59\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"hKCty7utJFyP","executionInfo":{"status":"ok","timestamp":1686679261811,"user_tz":-540,"elapsed":40,"user":{"displayName":"김민우","userId":"15651798176472178488"}}},"execution_count":30,"outputs":[]}]}